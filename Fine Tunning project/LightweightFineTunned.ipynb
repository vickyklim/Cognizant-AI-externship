{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#pip install datasets evaluate torch peft"],"metadata":{"id":"8zli82PN_xYo","executionInfo":{"status":"ok","timestamp":1722220819146,"user_tz":180,"elapsed":6,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["#pip install transformers"],"metadata":{"id":"rM4JDhG4QD1o","executionInfo":{"status":"ok","timestamp":1722220819146,"user_tz":180,"elapsed":5,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTbwS7n7MDN0","executionInfo":{"status":"ok","timestamp":1722220821276,"user_tz":180,"elapsed":2134,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}},"outputId":"b7f482fb-9e4c-4263-8188-2175be21a067"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["output_dir = \"/content/drive/MyDrive/Cognizant/results\""],"metadata":{"id":"ib0WvmPqMQnD","executionInfo":{"status":"ok","timestamp":1722220821276,"user_tz":180,"elapsed":3,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["# Lightweight Fine-Tuning Project"],"metadata":{"id":"OlhrWyNUA9uf"}},{"cell_type":"markdown","source":["TODO: In this cell, describe your choices for each of the following\n","\n","* PEFT technique: LoRA\n","* Model: GPT-2\n","* Evaluation approach: Evaluate method with a Hugging Face Trainer\n","* Fine-tuning dataset: climatebert/environmental_claims"],"metadata":{"id":"uPb86H7ZA_vf"}},{"cell_type":"markdown","source":["## Loading and Evaluating a Foundation Model\n"],"metadata":{"id":"GCXlKCh7BBsP"}},{"cell_type":"markdown","source":["TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."],"metadata":{"id":"mpxJEgqtZ9kd"}},{"cell_type":"code","source":["from transformers import GPT2Config, GPT2Tokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from datasets import load_dataset\n","import numpy as np\n","import random\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","\n","from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForSequenceClassification\n","random.seed(10)"],"metadata":{"id":"uewlLNKAaAdl","executionInfo":{"status":"ok","timestamp":1722220821276,"user_tz":180,"elapsed":2,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["import json"],"metadata":{"id":"EAO5iJudR59N","executionInfo":{"status":"ok","timestamp":1722220821276,"user_tz":180,"elapsed":2,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = AutoModelForSequenceClassification.from_pretrained('gpt2',\n","                                                               num_labels=2,\n","                                                               id2label={0: \"no\", 1: \"yes\"},\n","                                                               label2id={\"no\": 0, \"yes\": 1})\n","# Model recognizes padding\n","model.config.pad_token_id = model.config.eos_token_id\n","\n","# Load the dataset\n","splits = [\"train\", \"validation\"]\n","dataset = {split: load_dataset(\"climatebert/environmental_claims\", split=split) for split in splits}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZV19Fy6NaCzu","outputId":"a44cfbb2-e849-4c0a-ec20-ecf70d306bbf","executionInfo":{"status":"ok","timestamp":1722220823728,"user_tz":180,"elapsed":2454,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":65,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["#def remove_columns(dataset, columns):\n","#    return dataset.remove_columns(columns)\n","\n","#dataset = {split: remove_columns(load_dataset(\"RuyuanWan/Politeness_Disagreement\", split=split), ['disagreement_rate']) for split in splits}\n","\n"],"metadata":{"id":"TAF3HtVuf9If","executionInfo":{"status":"ok","timestamp":1722220823728,"user_tz":180,"elapsed":6,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["for split in splits:\n","    print(f\"Columns in {split} dataset:\", dataset[split].column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24oK0LUqgFKV","outputId":"7051c38d-13dd-4dce-a866-c7e993026c90","executionInfo":{"status":"ok","timestamp":1722220823728,"user_tz":180,"elapsed":6,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in train dataset: ['text', 'label']\n","Columns in validation dataset: ['text', 'label']\n"]}]},{"cell_type":"code","source":["def preprocess_function(examples):\n","\n","    print(\"Original text length:\", [len(text) for text in examples['text'][:5]])\n","    tokenized = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n","\n","    print(\"Tokenized input_ids length:\", [len(ids) for ids in tokenized['input_ids'][:5]])\n","    return tokenized\n","\n","encoded_dataset = {split: dataset[split].map(preprocess_function, batched=True) for split in splits}\n"],"metadata":{"id":"S4MElZ1JaFXt","executionInfo":{"status":"ok","timestamp":1722220829692,"user_tz":180,"elapsed":5969,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["for split in splits:\n","    print(f\"Lengths of tokenized sequences in {split} dataset:\")\n","    for i in range(5):  # Print lengths for the first 5 examples\n","        print(len(encoded_dataset[split][i]['input_ids']))\n","\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='binary', pos_label=1)\n","    recall = recall_score(labels, preds, average='binary', pos_label=1)\n","    f1 = f1_score(labels, preds, average='binary', pos_label=1)\n","\n","    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    specificity = tn / (tn + fp)\n","\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,  # Sensitivity\n","        \"specificity\": specificity,\n","        \"f1\": f1\n","    }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Sz82K0Po55P","outputId":"0517e89e-fed3-49fd-ddb2-ffaa4dd107a1","executionInfo":{"status":"ok","timestamp":1722220829694,"user_tz":180,"elapsed":18,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Lengths of tokenized sequences in train dataset:\n","128\n","128\n","128\n","128\n","128\n","Lengths of tokenized sequences in validation dataset:\n","128\n","128\n","128\n","128\n","128\n"]}]},{"cell_type":"code","source":["\n","training_args = TrainingArguments(\n","    output_dir= output_dir,\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Evaluate the model\n","eval_result = trainer.evaluate()\n","print(f\"Evaluation result: {eval_result}\")\n","\n","eval_results_file = f\"{output_dir}/evaluation_initial_results.json\"\n","with open(eval_results_file, 'w') as f:\n","    json.dump(eval_result, f)\n","print(f\"Evaluation results saved to: {eval_results_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"hvCfr5CzaIvU","outputId":"4ad5183a-bc0f-43a8-e37b-3987c3ea92b2","executionInfo":{"status":"ok","timestamp":1722220832331,"user_tz":180,"elapsed":2653,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [34/34 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation result: {'eval_loss': 0.5194663405418396, 'eval_accuracy': 0.7584905660377359, 'eval_precision': 1.0, 'eval_recall': 0.030303030303030304, 'eval_specificity': 1.0, 'eval_f1': 0.05882352941176471, 'eval_runtime': 2.3459, 'eval_samples_per_second': 112.963, 'eval_steps_per_second': 14.493}\n","Evaluation results saved to: /content/drive/MyDrive/Cognizant/results/evaluation_initial_results.json\n"]}]},{"cell_type":"markdown","source":["## Performing Parameter-Efficient Fine-Tuning\n"],"metadata":{"id":"lRhRCzGoOzx0"}},{"cell_type":"markdown","source":["TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.\n","\n","\n","\n"],"metadata":{"id":"gJ6IUuPKO0ik"}},{"cell_type":"code","source":["# Configure LoRA\n","config = LoraConfig(\n","    r=10,  # Rank\n","    lora_alpha=32,\n","    target_modules=['c_attn', 'c_proj'],\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS\n",")\n","\n","# Create PEFT model\n","peft_model = get_peft_model(model, config)\n","peft_model.print_trainable_parameters()\n","\n","# Initialize the Trainer with the PEFT model\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlM7dhg-YuwO","outputId":"0ec27b5a-9b32-4b48-a0f0-8690cc7aa66c","executionInfo":{"status":"ok","timestamp":1722220833622,"user_tz":180,"elapsed":1293,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":71,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 1,015,296 || all params: 125,456,640 || trainable%: 0.8093\n"]}]},{"cell_type":"code","source":["# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","eval_result_finetunned = trainer.evaluate()\n","print(f\"Evaluation result: {eval_result_finetunned}\")\n","\n","# Save the PEFT model weights\n","\n","eval_results_file_finetunned = f\"{output_dir}/evaluation_finetunned_results.json\"\n","with open(eval_results_file_finetunned, 'w') as f:\n","  json.dump(eval_result_finetunned, f)\n","print(f\"Evaluation results saved to: {eval_results_file_finetunned}\")\n","\n","# Save the peft_model to the specified directory in Google Drive\n","peft_model.save_pretrained(f'{output_dir}/peft_model')\n","print(f\"peft_model saved to: {output_dir}/peft_model\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"GwExxwKxY9ud","outputId":"96afbc4a-1a89-4e60-aa8e-3ab53f4df16d","executionInfo":{"status":"ok","timestamp":1722220973791,"user_tz":180,"elapsed":139700,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='795' max='795' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [795/795 02:16, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Specificity</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.374054</td>\n","      <td>0.837736</td>\n","      <td>0.632184</td>\n","      <td>0.833333</td>\n","      <td>0.839196</td>\n","      <td>0.718954</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.386600</td>\n","      <td>0.245449</td>\n","      <td>0.894340</td>\n","      <td>0.771429</td>\n","      <td>0.818182</td>\n","      <td>0.919598</td>\n","      <td>0.794118</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.386600</td>\n","      <td>0.223099</td>\n","      <td>0.898113</td>\n","      <td>0.767123</td>\n","      <td>0.848485</td>\n","      <td>0.914573</td>\n","      <td>0.805755</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [34/34 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation result: {'eval_loss': 0.2230994701385498, 'eval_accuracy': 0.8981132075471698, 'eval_precision': 0.7671232876712328, 'eval_recall': 0.8484848484848485, 'eval_specificity': 0.914572864321608, 'eval_f1': 0.8057553956834531, 'eval_runtime': 2.2578, 'eval_samples_per_second': 117.371, 'eval_steps_per_second': 15.059, 'epoch': 3.0}\n","Evaluation results saved to: /content/drive/MyDrive/Cognizant/results/evaluation_finetunned_results.json\n","peft_model saved to: /content/drive/MyDrive/Cognizant/results/peft_model\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"IWFnqPE5Ushg","executionInfo":{"status":"ok","timestamp":1722220973791,"user_tz":180,"elapsed":23,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["We see an improvement on accuracy, precision and specifity as we add epochs\n","\n","Also, we have more accuracy and less training loss than pre-fine tunning."],"metadata":{"id":"b2oE8Zg6TN95"}},{"cell_type":"markdown","source":["## Performing Inference with a PEFT Model"],"metadata":{"id":"O4jFR8nmcIRt"}},{"cell_type":"markdown","source":["TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."],"metadata":{"id":"hUSCAZFkcKv0"}},{"cell_type":"code","source":["# Load the tokenizer and the PEFT model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","peft_model = AutoPeftModelForSequenceClassification.from_pretrained('./peft_model',\n","                                                                num_labels=2,\n","                                                                id2label={0: \"no\", 1: \"yes\"},\n","                                                                label2id={\"no\": 0, \"yes\": 1})\n","peft_model.config.pad_token_id = peft_model.config.eos_token_id\n","\n","# Re-setup the Trainer with the PEFT model\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=encoded_dataset[\"train\"],\n","    eval_dataset=encoded_dataset[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Evaluate the PEFT model\n","peft_eval_result = trainer.evaluate()\n","print(f\"PEFT model evaluation result: {peft_eval_result}\")\n","\n","# Compare the results\n","initial_eval_accuracy = eval_result['eval_accuracy']\n","finetunned_eval_accuracy = eval_result_finetunned['eval_accuracy']\n","peft_eval_accuracy = peft_eval_result['eval_accuracy']\n","\n","print(f\"Initial model evaluation accuracy: {initial_eval_accuracy}\")\n","print(f\"Model evaluation after fine-tuning accuracy: {finetunned_eval_accuracy}\")\n","print(f\"PEFT model evaluation accuracy: {peft_eval_accuracy}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"NrDAbIsVcM64","outputId":"b1ae4d9f-a92b-41a5-ba31-678934a91a7a","executionInfo":{"status":"ok","timestamp":1722220976685,"user_tz":180,"elapsed":2910,"user":{"displayName":"Victoria Klimkowski","userId":"01708228906978695898"}}},"execution_count":73,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [34/34 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PEFT model evaluation result: {'eval_loss': 0.2672160863876343, 'eval_accuracy': 0.8754716981132076, 'eval_precision': 0.6987951807228916, 'eval_recall': 0.8787878787878788, 'eval_specificity': 0.8743718592964824, 'eval_f1': 0.778523489932886, 'eval_runtime': 2.2064, 'eval_samples_per_second': 120.107, 'eval_steps_per_second': 15.41}\n","Initial model evaluation accuracy: 0.7584905660377359\n","Model evaluation after fine-tuning accuracy: 0.8981132075471698\n","PEFT model evaluation accuracy: 0.8754716981132076\n"]}]},{"cell_type":"markdown","source":["we get more accuracy on the model after fine-tunning than on the initial one"],"metadata":{"id":"626yI3jXSm3c"}}]}